# Configure the hyper-parameters of the model.
model:
  dim: 768
  seq: 1024
  heads: 12
  layers: 12
  hidden_size: 2048
  vocab_size: 30000
  positional:
    name: rotatory # Options: [absolute, rotary]
    parameters:
      theta: 10000
  # Refer https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity
  activation:
    name: GELU # Options: [ReLU, LeakyReLU, PReLU, SiLU, GLU, GELU]. Default: GELU
    parameters:
      approximate: none
  # Refer https://pytorch.org/docs/stable/nn.html#normalization-layers
  norm:
    name: LayerNorm # Options: [LayerNorm, RMSNorm]. Defaul: LayerNorm
    parameters:
      eps: 1e-05
  # Share weights between embedding and output layer
  tie_weights: true
  # Enable Grouped Query Attention (default: disabled)
  # gqa:
  #   kv_heads: 4

# Configure the hyper-parameters of the optimization loop.
loop:
  lr: 3e-4 # Considered as initial learning rate when scheduling is enabled
  batch_size: 32
  optimizer:
    name: AdamW
    parameters:
      betas: [0.9, 0.99]
      eps: 1e-8
      weight_decay: 1e-2
      fused: true
  # Learning rate scheduler (default: disabled)
  # schedule:
  #   peak_lr: 3e-4
  #   warmup: 3000
  #   weight_decay: 0.05
