# Configuration file defining the hyper-parameters for GPT family of models.
# Customize it by modifying/uncommenting parts of the configuration.
model:
  emb: 768
  seq: 1024
  heads: 12
  layers: 12
  hidden_size: 2048
  vocab_size: 30000
  positional:
    name: RoPE # Options: [Sinusoid, Relative, RoPE]
    parameters:
      theta: 5000000
  activation:
    # Refer https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity
    name: GeLu # Options: [ReLU, LeakyReLU, PReLU, SiLU, GLU, GeLU]
    # Refer documentation of the activation to know about the parameters to pass to the activations
    parameters:
      approximate: none
  # Share weights between embedding and output layer
  tie_weights: true
  # Enable Grouped Query Attention (default: disabled)
  # gqa:
  #   kv_heads: 4

# Configuration used to train the model
train:
  lr: 3e-4 # Considered as initial learning rate when scheduling is enabled
  batch_size: 32
  optimizer:
    name: AdamW
    parameters:
      betas: [0.9, 0.99]
      eps: 1e-8
      weight_decay: 1e-2
      fused: true
  # Learning rate scheduler (default: disabled)
  # schedule:
  #   peak_lr: 3e-4
  #   warmup: 3000
